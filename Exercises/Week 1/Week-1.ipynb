{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CecD5oUo-6D5"
      },
      "source": [
        "# 📚 Exercise Session - Week 1\n",
        "\n",
        "Welcome to Week 1 exercise session of **CS-552: Modern NLP**!\n",
        "\n",
        "> **What will be covered:**\n",
        "> 1. [**TASK A:** Training a word embedding model](#1)\n",
        "> 2. [**TASK B:** Word embedding similarity](#2)\n",
        "> 3. [**TASK C:** Sentiment analysis of movie reviews](#3)\n",
        "\n",
        "> **By the end of the session you will be able to:**\n",
        "> - ✅  train from scratch a word embedding model\n",
        "> - ✅  load and use pre-trained word embeddings\n",
        "> - ✅  perform word embedding analogies and understand their implicit biases\n",
        "> - ✅  train and evaluate a sentiment classifier using word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORxpiv_C-6D6"
      },
      "source": [
        "## Intro - Word embedding models\n",
        "\n",
        "#### 🔵 What?\n",
        "A word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems.\n",
        "\n",
        "\n",
        "#### 🟡 How?\n",
        "There are different versions of word embeddings depending on the way the embeddings are learned.\n",
        "- Some of the models learn a focus word given the neighboring words that surround them\n",
        "- Some other models learn the neighboring words given the focus word.\n",
        "- Some other word embedding models try to derive the relationship between the words from global statistics of the words in the overall corpus.\n",
        "\n",
        "\n",
        "#### 🟣 Why?\n",
        "Word embedding is an important concept in NLP that is used for representing words for text analysis in the form of real-valued vectors. It is an advancement that has improved the ability of computers to understand text-based content in a better way.\n",
        "\n",
        "\n",
        "#### A little bit of history on word embeddings\n",
        "In 2013, Tomas Mikolov et al. developed an algorithm for learning word embeddings called **Word2vec** [[paper](https://arxiv.org/abs/1301.3781)][[code](https://code.google.com/archive/p/word2vec/)]. This algorithm uses a shallow neural network to learn word vectors so that the representation of each word of a given corpus is good at predicting its own contexts (Skip-Gram) or vice versa (CBOW).\n",
        "\n",
        "One year after the publication of Word2vec, Pennington et al. developed a new algorithm for learning word embeddings called **GloVe** [[paper](https://nlp.stanford.edu/pubs/glove.pdf)][[code](https://github.com/stanfordnlp/GloVe)]. This algorithm is based on the observation that word relationships can be recovered from the co-occurrence statistics of any (large enough) corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtiF-QD3-6D7"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid orange;background-color:#F3F3F3; color: black\">\n",
        "    <h3>Goal of today's exercise</h3><br/>\n",
        "In this exercise session, we will use both variants of these word embedding models (Word2Vec and GloVe) for our experiments.\n",
        "For the first part, we will train from scratch a Word2vec model with different datasets. In the second part, we will load pre-trained Word2vec and GloVe embeddings and test them on how well they capture concept similarity among words. Finally, for the last part, we will train a simple sentiment analysis classifier with these pre-trained embeddings as we discussed in the lecture.\n",
        "\n",
        "You will need to fill in all the missing pieces of code. For each one, you will be given a 🎯 `Goal` guidance that explains the steps you need to implement and a 💻 `API` reference with the functions or classes you can use to achieve each goal.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DbOkT-C1-6D8",
        "outputId": "4d7ec863-70b1-4c18-a790-55234cd6ceee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ],
      "source": [
        "!conda create -n \"mnlp_exercises\" python=3.10.11 --yes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ulqMKJT7-6D9",
        "outputId": "d50a81a5-50c1-4985-f340-ce3ca975b357",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ],
      "source": [
        "!conda activate mnlp_exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vsy2xrZC-6D9",
        "outputId": "9df1f9c1-0d9f-4438-f78c-83912fcc2edd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pandas gensim numpy matplotlib seaborn scikit-learn tqdm torch torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RgbeBhLF-6D-"
      },
      "outputs": [],
      "source": [
        "# imported libraries for this exercise\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.utils import tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTdG6nwL-6D-"
      },
      "source": [
        "<a id=\"1\"></a>\n",
        "## 1. TASK A: Training a word embedding model from stratch\n",
        "\n",
        "In this part, you will train a model to learn vector representations of words and more specifically, we will train the word2vec model.\n",
        "To train the model we will use the Gensim library which is a free open-source Python library for representing documents as semantic vectors.\n",
        "\n",
        "> **Gensim** is designed to process raw, unstructured digital texts (“plain text”) using unsupervised machine learning algorithms. The algorithms in Gensim, such as Word2Vec, FastText, Latent Semantic Indexing (LSI, LSA, LsiModel), Latent Dirichlet Allocation (LDA, LdaModel) etc, automatically discover the semantic structure of documents by examining statistical co-occurrence patterns within a corpus of training documents. These algorithms are unsupervised, which means no human input is necessary – you only need a corpus of plain text documents.\n",
        ">\n",
        "> You can discover all Gensim functionalities [here](https://radimrehurek.com/gensim/index.html).\n",
        "\n",
        "To train your embedding model, you will need to instantiate Word2Vec and pass the data to the model to train with. We essentially need to pass on a list of lists, where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary.\n",
        "\n",
        "**Data:** We will train our embedding model using a toy dataset that is provided by the Gensim library. You can use the dataset under the following path `gensim.test.utils.common_texts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-onUDIp3-6D_",
        "outputId": "be7f4b07-a63c-41e5-85be-2083dc36708b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human', 'interface', 'computer'],\n",
              " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
              " ['eps', 'user', 'interface', 'system'],\n",
              " ['system', 'human', 'system', 'eps'],\n",
              " ['user', 'response', 'time'],\n",
              " ['trees'],\n",
              " ['graph', 'trees'],\n",
              " ['graph', 'minors', 'trees'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# bag of words for each sentence in toy dataset (imported from the gensim library in cell 2)\n",
        "common_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZPEg7ZU-6D_"
      },
      "source": [
        "#### Train a Gensim Word2Vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HxK76R7-6D_"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "    \n",
        "- 🎯 **Goal:** Train with `common_texts` dataset a Gensim word2vec model.<br>\n",
        "\n",
        "- 💻  **API:** You can use `gensim.models.Word2Vec` class.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IWC-1C2O-6D_"
      },
      "outputs": [],
      "source": [
        "# instantiate and train model\n",
        "w2v_model =  Word2Vec(sentences= common_texts, min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sEdsL3H3-6EA",
        "outputId": "3e21f0fc-7887-44de-9df2-bcac9d4524e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'system': 0,\n",
              " 'graph': 1,\n",
              " 'trees': 2,\n",
              " 'user': 3,\n",
              " 'minors': 4,\n",
              " 'eps': 5,\n",
              " 'time': 6,\n",
              " 'response': 7,\n",
              " 'survey': 8,\n",
              " 'computer': 9,\n",
              " 'interface': 10,\n",
              " 'human': 11}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# dict with vocabulary key indexes\n",
        "words = w2v_model.wv.key_to_index\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2uusQ6f-6EA"
      },
      "source": [
        "#### Find word similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c8V8Ljbr-6EA",
        "outputId": "7fa020ec-0aa9-479e-dfc0-f499277d8c5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vector length of the word is 100.\n"
          ]
        }
      ],
      "source": [
        "# explore embeddings\n",
        "vector = w2v_model.wv['computer']  # get numpy vector of a word\n",
        "print('The vector length of the word is {}.'.format(len(vector)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbPoAd3n-6EA"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** Use the trained word2vec model to find most similar words for token _\"computer\"_.\n",
        "- 💻 **API:** You can use Gensim's `.most_similar` function.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4-Bt_c4b-6EA",
        "outputId": "f22774b9-347c-48bc-e26d-bc98d1d74b8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('system', 0.21617139875888824),\n",
              " ('user', -0.16911619901657104),\n",
              " ('eps', -0.10513808578252792),\n",
              " ('graph', -0.09575342386960983),\n",
              " ('response', -0.09317591041326523),\n",
              " ('human', -0.07424270361661911),\n",
              " ('survey', 0.04468922317028046),\n",
              " ('trees', -0.03284316882491112),\n",
              " ('interface', 0.015203381888568401),\n",
              " ('time', 0.0019510635174810886)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# print most similar words\n",
        "most_similar = w2v_model.wv.most_similar('computer')\n",
        "\n",
        "sorted_by_second = sorted(most_similar, key=lambda tup: abs(tup[1]), reverse=True)\n",
        "\n",
        "sorted_by_second"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9XAR4UX-6EB"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid gray;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "#### Play around with inital settings\n",
        "\n",
        "- Re-train the model with different datasets ( [Wikipedia](https://radimrehurek.com/gensim/corpora/wikicorpus.html), [Opinosis](https://radimrehurek.com/gensim/corpora/opinosiscorpus.html) ) and vector sizes and run similarity with different top-n. How would you comment on the results?\n",
        "- Think about the limitations of the Word2vec approach.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK9QEsLa-6EB"
      },
      "source": [
        "***--- YOU CAN WRITE YOU ANSWER HERE ---***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0sHL-UF-6EB"
      },
      "source": [
        "<a id=\"2\"></a>\n",
        "## 2. TASK B: Word embedding similarity\n",
        "\n",
        "For this task, we will use pre-trained word embedding models.\n",
        "\n",
        "**Word2Vec:** We will use once again Word2vec as in the previous question, but in this case we will load the pre-trained version of it that has been trained on a part of the Google News dataset (about 100 billion words). The model variant that we will use, contains 300-dimensional vectors for 3 million words and phrases.\n",
        "\n",
        "**GloVe:** We will also use GloVe word embeddings that have been pre-trained on 2B tweets, 27B tokens, 1.2M vocab, uncased. The model variant that we will use contains 100-dimensional vectors.\n",
        "\n",
        "Once we load the vector represenation of words, we will:\n",
        "- find similarities between group of words.\n",
        "- discover a very cool property of word embeddings through analogies.\n",
        "- evaluate their performance on how well they capture the semantics between words on the SimLex task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCw-i1js-6EB"
      },
      "source": [
        "### 2.1 Using pre-trained word embedding models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "95FZLZ6i-6EB",
        "outputId": "164974d3-7c46-4600-c87e-cf2d643c47f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ],
      "source": [
        "# Show all available models in gensim-data\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dTzrg_L-6EB",
        "outputId": "9371e38f-d346-469d-f6ab-441830bf1955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=====---------------------------------------------] 10.5% 174.2/1662.8MB downloaded"
          ]
        }
      ],
      "source": [
        "# Download the \"word2vec-google-news-300\" embeddings\n",
        "w2v_vectors = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4mVJ88u-6EB"
      },
      "outputs": [],
      "source": [
        "# Download the \"glove-wiki-gigaword-100\" embeddings\n",
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxUWaOhm-6EB"
      },
      "source": [
        "The best way to understand a data is by visualizing them. We will use t-SNE to reduce the data with 300 dimensions to 2 dimensions, so that we can plot the data on our screens.\n",
        "t-SNE is a non-linear dimensionality reduction algorithm that attempts to represent high-dimensional data and the underlying relationships between vectors in a lower-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zAdCrS2-6EC"
      },
      "outputs": [],
      "source": [
        "# visualization utility function. NO NEED TO CHANGE ANYTHING HERE\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_theme(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "def tsne_scatterplot(model, search_word, topn):\n",
        "    labels = [search_word]\n",
        "    tokens = [model[search_word]]\n",
        "    similar = [1]\n",
        "    close_words = model.similar_by_word(search_word, topn=topn)\n",
        "    for word in close_words:\n",
        "        tokens.append(model[word[0]])\n",
        "        labels.append(word[0])\n",
        "        similar.append(word[1])\n",
        "\n",
        "    tsne_model = TSNE(n_components=2, perplexity=topn, init='pca')\n",
        "    coordinates = tsne_model.fit_transform(np.array(tokens))\n",
        "    df = pd.DataFrame({'x': [x for x in coordinates[:, 0]],\n",
        "                       'y': [y for y in coordinates[:, 1]],\n",
        "                       'words': labels,\n",
        "                       'similarity': similar}\n",
        "                      )\n",
        "    fig, ax = plt.subplots()\n",
        "    plot = ax.scatter(df.x, df.y, c=df.similarity, cmap='Reds')\n",
        "    for i in range(len(df)):\n",
        "        ax.annotate(\"  {} ({:.2f})\".format(df.words[i].title(),\n",
        "                                           df.similarity[i]),\n",
        "                    (df.x[i], df.y[i]))\n",
        "\n",
        "    plt.colorbar(mappable=plot, ax=ax)\n",
        "    plt.title('t-SNE visualization for {}'.format(search_word))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCePF7Ue-6EC"
      },
      "outputs": [],
      "source": [
        "# visualize the most similar words for a given word.\n",
        "tsne_scatterplot(glove_vectors, \"computer\", 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQfouAt2-6EC"
      },
      "source": [
        "#### Interactive visualization\n",
        "\n",
        "In the following link, you can explore the state-of-the-art models in an interactive embedding visualization: https://projector.tensorflow.org/.\n",
        "\n",
        "Using the TensorBoard Embedding Projector, you can graphically represent high dimensional embeddings. This can be helpful in visualizing, examining, and understanding your embeddings.\n",
        "\n",
        "Give it a try! Test interesting clusters of similar words!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_ScsYAf-6EC"
      },
      "source": [
        "### 2.2 Check word analogies\n",
        "\n",
        "The fact that we can analyze the use of words in language to deduce their meaning is a fundamental idea of [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics). This is the inspiration behind many algorithms for learning numerical representations of words (word embeddings). An important aspect of these representations is the ability to solve word analogies of the form “A is to B what C is to X” using simple arithmetic. This is generally simplified as “King — Man + Woman $\\sim$ Queen.\n",
        "\n",
        "In this subquestion we will explore interesting relations that can be extracted from word embeddings. You will be given semantic relations and you need to implement the respective code using the embedding model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "925XNFtZ-6EC"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** Use the pre-trained GloVe embeddings to implement the following analogies.\n",
        "- 💻 **API:** You can use Gensim's `.most_similar` function.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTXwnj1L-6EC"
      },
      "source": [
        "**Analogy A:**  `King` — `Man` + `Woman` ~ `?`\n",
        "\n",
        "`?`: `Queen`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba5GocxI-6ED"
      },
      "outputs": [],
      "source": [
        "# Check the \"most similar words\", using the default \"cosine similarity\" measure.\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJTTYlGy-6ED"
      },
      "source": [
        "**Analogy B:**  `France` — `Paris` + `Athens` ~ `?`\n",
        "\n",
        "`?`: `Greece`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6ZBYOfL-6ED"
      },
      "outputs": [],
      "source": [
        "# Check the \"most similar words\", using the default \"cosine similarity\" measure.\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IcLs8qO-6ED"
      },
      "source": [
        "However, we should not forget that these models are trained in human generated language. Meaning that sociatital and demographic biases are inevitable. In the following example you will unfortunatelly witness these kind of biases that the model unitentionally proragates to its final output.\n",
        "\n",
        "Fortunately, extensive reaserch has been developed the last years to tackle this issue trying to de-bias the embeddings models. More regarding this topic will be covered in Week 11  when we will be talking about Ethics in NLP regarding Toxicity, Bias and Fairness.\n",
        "\n",
        "**Analogy C:**  `Doctor` - `Man` + `Woman` ~ `Nurse`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tihtlwsZ-6ED"
      },
      "outputs": [],
      "source": [
        "# Check the \"most similar words\", using the default \"cosine similarity\" measure.\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3-8uqza-6ED"
      },
      "source": [
        "### 2.3 Evaluate different models with SimLex\n",
        "\n",
        "In this part of the exercise, we will evaluate the syntactic concreteness of the created vectors using their similarity. For this task, we will use the SimLex999 dataset which is a human-annotated dataset regarding the similarity of concepts of words.\n",
        "\n",
        "- 📄 Paper: https://aclanthology.org/J15-4004.pdf\n",
        "- 👾 More about the task: https://fh295.github.io/simlex.html\n",
        "\n",
        "> - Download the data from [here](https://fh295.github.io/SimLex-999.zip)\n",
        "> - Unzip the file and put the corresponding folder with the dataset in the same dir as this notebook\n",
        "\n",
        "SimLex-999 is a gold standard resource for the evaluation of models that learn the meaning of words and concepts. SimLex-999 provides a way of measuring how well models capture similarity, rather than relatedness or association.\n",
        "\n",
        "#### Dataset:\n",
        "SimLex-999.txt is a tab-separated plaintext file, where rows correspond to concept pairs and columns correspond to the properties of each pair.\n",
        "\n",
        "|Column | Description |\n",
        "|-------|:-------------|\n",
        "|word1| The first concept in the pair.|\n",
        "|word2| The second concept in the pair. Note that the order is only relevant to the column Assoc(USF). These values (free association scores) are asymmetric. All other values are symmetric properties independent of the ordering word1, word2.|\n",
        "|POS| The majority part-of-speech of the concept words, as determined by occurrence in the POS-tagged British National Corpus. Only pairs of matching POS are included in SimLex-999.|\n",
        "|SimLex999 | The SimLex999 similarity rating. Note that average annotator scores have been (linearly) mapped from the range [0,6] to the range [0,10] to match other datasets such as WordSim-353. |\n",
        "|conc(w1)| The concreteness rating of word1 on a scale of 1-7. Taken from the University of South Florida Free Association Norms database. |\n",
        "|conc(w2)| The concreteness rating of word2 on a scale of 1-7. Taken from the University of South Florida Free Association Norms database.|\n",
        "|concQ| The quartile the pair occupies based on the two concreteness ratings. Used for some analyses in the above paper.|\n",
        "|Assoc(USF)| The strength of free association from word1 to word2. Values are taken from the University of South Florida Free Association Dataset.|\n",
        "|SimAssoc333| Binary indicator of whether the pair is one of the 333 most associated in the dataset (according to Assoc(USF)). This subset of SimLex999 is often the hardest for computational models to capture because the noise from high association can confound the similarity rating. See the paper for more details.|\n",
        "|SD(SimLex)| The standard deviation of annotator scores when rating this pair. Low values indicate good agreement between the 15+ annotators on the similarity value SimLex999. Higher scores indicate less certainty. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSDwEgBJ-6EE"
      },
      "source": [
        "We will be using the **`SimLex999` similarity column** to evaluate our word embeddings on this dataset. We will select the top 50 most similar pairs according to simlex999 column which a score from 0 to 10 regarding the similarities of the two words (columns: word1, word2) and it has been annotated by humans. On this subset of data, we will find the similarity score of the two words using our two pre-trained embedding models (Word2Vec, GloVe)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0t87_S_-6EE"
      },
      "outputs": [],
      "source": [
        "# read data\n",
        "simlex = pd.read_csv('SimLex-999/SimLex-999.txt', delimiter='\\t')\n",
        "simlex = simlex[['word1', 'word2', 'SimLex999']]\n",
        "simlex.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSkC4fRo-6EF"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** Print the min, avg and max values of SimLex999 score.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxP_0Zxa-6EF"
      },
      "outputs": [],
      "source": [
        "# stats on the SimLex999 similarity score\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKTvvZ4x-6EG"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** Select the top 50 pairs with the highest SimLex similarity score.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9K5tWQx-6EH"
      },
      "outputs": [],
      "source": [
        "# select a subset of word-pairs with the highest similiarity score given by annotators\n",
        "# simlex_similar_pairs = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QgAWVPU-6EH"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** For each word pair from `simlex_similar_pairs`, compute the similarity between the GloVe embeddings (also Word2Vec embeddings) of the words.\n",
        "- 💻 **API:** You can use Gensim's `.similarity` function.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWHYVZ41-6EH"
      },
      "outputs": [],
      "source": [
        "# computes the similarities of each pair using each word embedding model\n",
        "# similarities = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6jlnRiU-6EH"
      },
      "outputs": [],
      "source": [
        "similarities.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq3pHopi-6EI"
      },
      "outputs": [],
      "source": [
        "# plots the distribution of similarity scores for each model\n",
        "sims_w2v = similarities[['similarity_w2v']].rename(columns = {'similarity_w2v': 'similarity'})\n",
        "sims_w2v['model'] = 'Word2vec'\n",
        "sims_glove = similarities[['similarity_glove']].rename(columns = {'similarity_glove': 'similarity'})\n",
        "sims_glove['model'] = 'GloVe'\n",
        "to_plot = pd.concat([sims_w2v, sims_glove], ignore_index=True)\n",
        "sns.displot(to_plot, x=\"similarity\", hue=\"model\", kind=\"kde\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SeQ-l7Y-6EI"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid gray;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "#### Extra quiz:\n",
        "Think about which model seems to perform better on these high-similarity examples and why based on your knowledge of how the embedding models have been trained.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7K96UqF-6EI"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScF2pk-O-6EI"
      },
      "source": [
        "<a id=\"3\"></a>\n",
        "## 3. TASK C: Sentiment analysis of movie reviews\n",
        "\n",
        "For this part, we will be using only the GloVe pre-trained embeddings.\n",
        "\n",
        "**Data:** We will build a sentiment classification model using the IMDB review dataset.\n",
        "\n",
        "> - Download the dataset [here](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/download?datasetVersionNumber=1)\n",
        "> - Save the .csv file in the same dir as this notebook.\n",
        "\n",
        "The IMDB dataset has 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing.\n",
        "\n",
        "The goal is to predict if a review is positive or negative using a classification (potentiall deep learning) algorithm.\n",
        "\n",
        "For more dataset information, please go through the following link,\n",
        "http://ai.stanford.edu/~amaas/data/sentiment/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHonfzgP-6EI"
      },
      "source": [
        "![IMDB-reviews-dataset-stats.png](attachment:IMDB-reviews-dataset-stats.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGb7Hyra-6EI"
      },
      "source": [
        "### 3.1 Dataset exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFDG9PFV-6EI"
      },
      "outputs": [],
      "source": [
        "#read dataset\n",
        "movie_reviews = pd.read_csv('IMDB Dataset.csv')\n",
        "movie_reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FLZGGCU-6EJ"
      },
      "outputs": [],
      "source": [
        "print('Number of movie reviews: {}'.format(len(movie_reviews)))\n",
        "classes = movie_reviews['sentiment'].unique()\n",
        "print('Number of classes: {} with values: {}'.format(len(classes), classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGjTORgg-6EJ"
      },
      "outputs": [],
      "source": [
        "# class distribution\n",
        "positive = len(movie_reviews[movie_reviews['sentiment']=='positive'])\n",
        "negative = len(movie_reviews[movie_reviews['sentiment']=='negative'])\n",
        "\n",
        "print('# of positive samples: {}'.format(positive))\n",
        "print('# of negative samples: {}'.format(negative))\n",
        "\n",
        "sns.set_theme(rc={'figure.figsize':(5.7,5.27)})\n",
        "sns.countplot(x=movie_reviews['sentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq0bWB8e-6EJ"
      },
      "source": [
        "### 3.2 Data preprocessing\n",
        "\n",
        "To build our classifier, we will leverage the pre-trained word embeddings (GloVe). Going from word embeddings to sentence vectors, we will need to find a way to take into account the embeddings of all the words each review contains. We will apply 2 techniques to do so: average and max pooling.  \n",
        "\n",
        "We will perform the following preprocessing steps:\n",
        "- Change the classes from str (negative, positive) to binary `(0, 1)`.\n",
        "- Get word embeddings and calculate the sentence vectors with both max and avg pooling.\n",
        "- Create a `pytorch.Dataset` instance of the IMDB data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoMfEQrJ-6EJ"
      },
      "source": [
        "#### From class names to binary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ55JPkV-6EJ"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** Change the classe names from str (negative, positive) to binary `(0, 1)` and add it as a new column to the exising dataframe.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HixiKFI2-6EJ"
      },
      "outputs": [],
      "source": [
        "# classes to binary\n",
        "# movie_reviews['target'] = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OnYamLb-6EK"
      },
      "source": [
        "#### From word embeddings to review embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daVlmrfi-6EK"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** For each review in the dataset, calculate the review vector by pooling from the review's word embeddings. Implement both max and avg pooling.\n",
        "- 💻 **API:** You can use numpy functions to manipulate the arrays. For review tokenization you can use `gensim.utils.tokenize` function.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scLcj7V3-6EK"
      },
      "outputs": [],
      "source": [
        "# apply embedding model to the words of the reviews and then perform pooling\n",
        "\n",
        "# review_vectors_max = np.array(...\n",
        "# review_vectors_avg = np.array(..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHl1EUVK-6EK"
      },
      "source": [
        "#### Pytorch Dataset class for the imported data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnOaUb1I-6EK"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** Create a pytorch Dataset class with the input data. Instantiate this class with the input data.\n",
        "- 💻 **API:** You can use `torch.utils.data.Dataset` class.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4aXFi8t-6EK"
      },
      "outputs": [],
      "source": [
        "# class IMDBreviews(Dataset):\n",
        "#     def __init__(self,...):\n",
        "#         ...\n",
        "\n",
        "#     def __len__(self):\n",
        "#         ...\n",
        "\n",
        "#     def __getitem__(self, ...):\n",
        "#         ...\n",
        "\n",
        "# dataset = IMDBreviews(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WmjlEPc-6EK"
      },
      "outputs": [],
      "source": [
        "# print tensors for sanity check\n",
        "print('Total samples {} with sentence embedding size: {}'.format(dataset.x.shape[0], dataset.x.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv67_3lr-6EL"
      },
      "source": [
        "#### Splitting dataset to train and test subsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DecWpGp-6EL"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** Split the dataset into train and test tests (80-20% splits).\n",
        "- 💻 **API:** You can use `torch.utils.data.random_split` function.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga44JqeP-6EL"
      },
      "outputs": [],
      "source": [
        "# split to train and test sets\n",
        "# train, test = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "povcysnm-6EL"
      },
      "outputs": [],
      "source": [
        "print('Train size: {}'.format(len(train)))\n",
        "print('Test size: {}'.format(len(test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxMckjfw-6EL"
      },
      "source": [
        "### 3.3 Train & evaluate a classifier on the movie reviews\n",
        "\n",
        "Once we have proprocessed the data, we will create the training and evaluation pipelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ezDUEcA-6EL"
      },
      "source": [
        "#### Pytorch Module class for model setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4ZZqydX-6EM"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** Create a pytorch model class. The architecture of the model can be as simple as a Linear layer along with a sigmoid activation function, or a more complicated deep architecture. Play around with different settings and architectures.\n",
        "- 💻 **API:** You can use `torch.nn.Module` class.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qllZgttb-6EM"
      },
      "outputs": [],
      "source": [
        "# create a pytorch nn.Module class for the simple logistic regression model\n",
        "# class SentimentAnalysisModel(nn.Module):\n",
        "\n",
        "#     def __init__(self, ...):\n",
        "#         super(SentimentAnalysisModel, self).__init__()\n",
        "#         ...\n",
        "\n",
        "#     def forward(self, ...):\n",
        "#         ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5GrtrLR-6EM"
      },
      "outputs": [],
      "source": [
        "model = SentimentAnalysisModel(dataset.x)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbTFI391-6EM"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g0wPAoj-6EM"
      },
      "source": [
        "#### Pytorch Data loader class for dataset batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bikTLCyA-6EM"
      },
      "outputs": [],
      "source": [
        "# create data loader\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader= DataLoader(test, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsp4A0ux-6EM"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJa_Ipwh-6EN"
      },
      "outputs": [],
      "source": [
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "from torch import optim\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtkwZhTY-6EN"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** Create the training loop: for each epoch, do the forward pass, compute the loss and do the backward propagation.\n",
        "- 💻 **API:** Check the official API for the [training process](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#train-the-network).\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIMwLJ2z-6EN"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "# model.train()\n",
        "# for epoch in range(10):\n",
        "#     epoch_loss = 0\n",
        "#     for i, data in enumerate(train_loader, 0):\n",
        "#         # get the inputs\n",
        "#         ...\n",
        "\n",
        "#         # Forward pass: Compute predicted y by passing x to the model\n",
        "#         ...\n",
        "\n",
        "#         # Compute loss\n",
        "#         ...\n",
        "\n",
        "#         # Perform a backward pass, and update the weights.\n",
        "#         ...\n",
        "#     print(f'Epoch {epoch + 1} | Loss: {(epoch_loss / len(train_loader)):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRUx7lFE-6EN"
      },
      "source": [
        "#### Inference on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhq0yykN-6EN"
      },
      "source": [
        "\n",
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid #03befc;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "- 🎯 **Goal:** Evaluate your model on the test set: for each batch of data, do the forward pass and compute the loss and the accuracy score.\n",
        "- 💻 **API:** Check the official API for the [testing process](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#test-the-network-on-the-test-data).\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJIJXlIR-6EN"
      },
      "outputs": [],
      "source": [
        "# batch_loss = 0\n",
        "# batch_acc = 0\n",
        "\n",
        "# model.eval()\n",
        "# for i, data in enumerate(test_loader, 0):\n",
        "#     # get the inputs\n",
        "#     ...\n",
        "\n",
        "#     # run forward pass\n",
        "#     ...\n",
        "\n",
        "#     # Compute loss\n",
        "#     ...\n",
        "\n",
        "#     # Compute accuracy\n",
        "#     acc = BinaryAccuracy()\n",
        "#     ...\n",
        "\n",
        "#     batch_loss += test_loss.item()\n",
        "#     batch_acc += acc.item()\n",
        "\n",
        "# test_loss = batch_loss / len(test_loader)\n",
        "# test_acc = batch_acc / len(test_loader)\n",
        "\n",
        "# print(f'Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7vbtC1S-6EO"
      },
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid gray;background-color:#F3F3F3; color: black\">\n",
        "\n",
        "#### Extra quiz:\n",
        "Play around with different settings (e.g. learning rate, batch size, epochs) and different architectures. Additionally try different pooling methods. Think how your design choices affect model performance on both train and test sets.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnuARcGw-6EO"
      },
      "source": [
        "### 🎉 Congrats! You finished your first exercise session!\n",
        "\n",
        "Awesome work! See you next week! 👋"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8ced340a52f9326f5856e1d63a73f97bd9f0a225610b549ff7b502d766a19ce"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}